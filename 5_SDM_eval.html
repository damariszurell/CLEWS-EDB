<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>SDM assessment and prediction</title>

<script src="site_libs/header-attrs-2.19/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Ecosystem dynamics and biodiversity</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Pracs: Spatial data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="0_setup.html">0. Getting started</a>
    </li>
    <li>
      <a href="1_SpatialData.html">1. Spatial data in R</a>
    </li>
    <li>
      <a href="2_BiodivData.html">2. Biodiversity data in R</a>
    </li>
    <li>
      <a href="3_EnvData.html">3. Environmental data in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Pracs: Species distribution models
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="4_SDM_intro.html">4. SDMs: simple model fitting</a>
    </li>
    <li>
      <a href="5_SDM_eval.html">5. SDMs: assessment and prediction</a>
    </li>
    <li>
      <a href="6_SDM_algorithms.html">6. SDMs: algorithms and ensembles</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Pracs: population modelling
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="7_RS_intro.html">7. Getting started with RangeShiftR</a>
    </li>
    <li>
      <a href="8_RS_lynx.html">8. RangeShiftR: Eurasian lynx reintroduction</a>
    </li>
    <li>
      <a href="9_RS_grouse.html">9. RangeShiftR: Black grouse range dynamics</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Pracs: ecosystem modelling
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="10_Mad_intro.html">10. Running MadingleyR</a>
    </li>
    <li>
      <a href="11_Mad_carnivores.html">11. MadingleyR: Role of large carnivores</a>
    </li>
    <li>
      <a href="12_Mad_landuse.html">12. MadingleyR: land use change effects</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://www.uni-potsdam.de/en/ibb-macroecology/index">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/ZurellLab">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">SDM assessment and prediction</h1>

</div>


<div class="alert alert-info">
<p><strong>RStudio project</strong></p>
<p>Open the RStudio project that we created in the first session. I
recommend to use this RStudio project for the entire course and within
the RStudio project create separate R scripts for each session.</p>
<ul>
<li>Create a new empty R script by going to the tab “File”, select “New
File” and then “R script”</li>
<li>In the new R script, type
<code># Session 5: SDM assessment and prediction</code> and save the
file in your folder “scripts” within your project folder, e.g. as
“5_SDM_eval.R”</li>
</ul>
</div>
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>We have already fitted simple species distribution models (SDMs) in
the previous tutorial. Remember the five general model building steps:
(i) conceptualisation, (ii) data preparation, (iii) model fitting, (iv)
model assessment, and (v) prediction <span class="citation">(Zurell et
al. 2020)</span>. In this practical, we will concentrate on steps
(iv)-(v). We will learn how to validate SDMs, visualise the fitted
species-environment relationships, and make predictions. For getting a
deeper understanding of these single steps, I highly recommend studying
more advanced reviews <span class="citation">(Guisan and Zimmermann
2000; Guisan and Thuiller 2005; Elith and Leathwick 2009)</span> and
textbooks on SDMs <span class="citation">(Franklin 2010; Guisan,
Thuiller, and Zimmermann 2017)</span>.</p>
<div id="recap-of-last-session-data-and-models" class="section level2"
number="1.1">
<h2><span class="header-section-number">1.1</span> Recap of last
session: data and models</h2>
<p>We will continue to work on the lynx example of the previous session,
based on the IUCN range maps. We can quickly read the data back in:</p>
<pre class="r"><code>library(terra)

load(&#39;data/lynx_thinned.RData&#39;)</code></pre>
<p>Quickly set up the model according to session 4.</p>
<pre class="r"><code>m_step &lt;- step(
  glm( occ ~ bio11 + I(bio11^2) + bio10 + I(bio10^2), 
               family=&#39;binomial&#39;, data=lynx_thinned)
  )</code></pre>
<pre><code>## Start:  AIC=266.1
## occ ~ bio11 + I(bio11^2) + bio10 + I(bio10^2)
## 
##              Df Deviance    AIC
## - I(bio11^2)  1   256.19 264.19
## &lt;none&gt;            256.10 266.10
## - bio10       1   271.31 279.31
## - I(bio10^2)  1   273.32 281.32
## - bio11       1   295.88 303.88
## 
## Step:  AIC=264.19
## occ ~ bio11 + bio10 + I(bio10^2)
## 
##              Df Deviance    AIC
## &lt;none&gt;            256.19 264.19
## - bio10       1   272.15 278.15
## - I(bio10^2)  1   274.09 280.09
## - bio11       1   352.75 358.75</code></pre>
<pre class="r"><code>summary(m_step)</code></pre>
<pre><code>## 
## Call:
## glm(formula = occ ~ bio11 + bio10 + I(bio10^2), family = &quot;binomial&quot;, 
##     data = lynx_thinned)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4470  -0.5948  -0.0410   0.5730   3.9948  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -12.02724    3.05541  -3.936 8.27e-05 ***
## bio11        -0.34456    0.04350  -7.921 2.35e-15 ***
## bio10         1.47632    0.39156   3.770 0.000163 ***
## I(bio10^2)   -0.04900    0.01263  -3.878 0.000105 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 444.40  on 322  degrees of freedom
## Residual deviance: 256.19  on 319  degrees of freedom
## AIC: 264.19
## 
## Number of Fisher Scoring iterations: 6</code></pre>
</div>
</div>
<div id="model-assessment" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Model assessment</h1>
<p>Before we can use our model for making predictions in space and time,
we need to assess model behaviour and predictive performance.</p>
<div id="visualising-response-curves" class="section level2"
number="2.1">
<h2><span class="header-section-number">2.1</span> Visualising response
curves</h2>
<p>When only looking at parameter estimates, it is sometimes difficult
to envision how exactly the fitted response (the “niche” function) looks
like. Also, for more complicated machine learning algorithms (that we
will get to know later), there are no parameter estimates to look at, so
we need different means to judge the plausibility of the fitted
response.</p>
<p>Response curves and response surfaces visualize the (mean) values
that a model would predict for an environmental situation, meaning for
specific values of the predictor variables. In the current example, we
use only two predictors, so we can simply construct a 3D surface that
shows the predicted values along the two environmental gradients.</p>
<p>We can get the predicted values using the <code>predict()</code>
function.</p>
<pre class="r"><code># If we do not provide &quot;newdata&quot;, then predict() should simply return the fitted values: 
head(predict(m_step, type=&#39;response&#39;))</code></pre>
<pre><code>##           1           2           3           4           5           6 
## 0.014108854 0.033726463 0.057589550 0.001026094 0.005222577 0.016571959</code></pre>
<pre class="r"><code>head(m_step$fitted)</code></pre>
<pre><code>##           1           2           3           4           5           6 
## 0.014108854 0.033726463 0.057589550 0.001026094 0.005222577 0.016571959</code></pre>
<p>If we want to predict model response along the two environmental
gradients, we first need to define a grid that contains all combinations
of the two variables. For this, we use a combination of the
<code>expand.grid()</code> and <code>seq()</code> functions.</p>
<pre class="r"><code># Wwe want to make predictions for all combinations of the two predictor variables
# and along their entire environmental gradients:
xyz &lt;- expand.grid(
  # We produce a sequence of environmental values within the predictor ranges:
    bio11 = seq(min(lynx_thinned$bio11),max(lynx_thinned$bio11),length=50),
    bio10 = seq(min(lynx_thinned$bio10),max(lynx_thinned$bio10),length=50)
    )

# Now we can make predictions to this new data frame
xyz$z &lt;- predict(m_step, newdata=xyz, type=&#39;response&#39;)
summary(xyz)</code></pre>
<pre><code>##      bio11             bio10              z            
##  Min.   :-14.448   Min.   : 7.446   Min.   :0.0000000  
##  1st Qu.: -8.352   1st Qu.:13.643   1st Qu.:0.0009263  
##  Median : -2.003   Median :20.099   Median :0.0342420  
##  Mean   : -2.003   Mean   :20.099   Mean   :0.2331589  
##  3rd Qu.:  4.347   3rd Qu.:26.555   3rd Qu.:0.4140808  
##  Max.   : 10.442   Max.   :32.752   Max.   :0.9832289</code></pre>
<pre class="r"><code># As result, we have a 3D data structure and want to visualise this.
# Here, I first set a color palette
library(RColorBrewer)
cls &lt;- colorRampPalette(rev(brewer.pal(11, &#39;RdYlBu&#39;)))(100)

# Finally, we plot the response surface using the wireframe function from the lattice package
library(lattice)
wireframe(z ~ bio11 + bio10, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), zlim = c(0, 1))</code></pre>
<p><img src="5_SDM_eval_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code># We can also rotate the axes to better see the surface
wireframe(z ~ bio11 + bio10, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), zlim = c(0, 1), 
          screen=list(z = -120, x = -70, y = 3))</code></pre>
<p><img src="5_SDM_eval_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<p>This looks very nice. However, if the model gets more complicated and
contains more variables than just two, how can we then visualise it?</p>
<p>One way is to cut the response surface into slices. Most often, we
simply plot the response along one environmental gradient while keeping
all other gradients constant at their mean. We call this
<strong><em>partial response plots</em></strong>. For simplicity, we can
use the function <code>partial_response()</code> from the
<code>mecofun</code> package for plotting.</p>
<pre class="r"><code>library(mecofun)

# Names of our variables:
my_preds &lt;- c(&#39;bio11&#39;, &#39;bio10&#39;)

# We want two panels next to each other:
par(mfrow=c(1,2))

# Plot the partial responses
partial_response(m_step, predictors = lynx_thinned[,my_preds])</code></pre>
<p><img src="5_SDM_eval_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>When you compare the response surface and the partial response
curves, you see that the latter only give us an incomplete picture of
what is going on as they are only “slicing” the response surface in the
middle.</p>
<div class="alert alert-info">
<p><em><strong>Exercise:</strong></em></p>
<p>Fit another GLM with two different (weakly correlated) predictor
variables and call this object <code>m2</code>. Plot the response
surface, and the partial response curves.</p>
<ul>
<li>How do you interpret the fitted relationship?</li>
</ul>
</div>
</div>
<div id="assessing-sdm-performance" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Assessing SDM
performance</h2>
<p>In the previous session, we already learned about the measure
“explained deviance” that tells us something about the goodness-of-fit,
meaning how well the model fits the data.</p>
<pre class="r"><code>expl_deviance(obs = lynx_thinned$occ,
              pred = m_step$fitted)</code></pre>
<pre><code>## [1] 0.4235096</code></pre>
<p>Here, our two-predictor model explains roughly 40% of the variance in
the data. However, often we are not only interested in how well our
model fits the data but how robust the model is against changes in the
input data and, thus, how robust predictions to other places and times
might be. This assessment of model performance is often referred to as
validation or evaluation. Evaluating the model on the calibration or
training data is often referred to as internal validation
(“resubstitution”) <span class="citation">(Araujo et al. 2005)</span>.
This generally gives a too optimistic picture of model performance. It
is better to evaluate the model using data that have not been used for
model fitting. One way is to split the data and only train the model on
some proportion of the data and validate it on the hold-out data.
Potential procedures are repeated split-samples (e.g. splitting into 70%
training and 30% test data and repeat several times) and <em>k</em>-fold
cross-validation (e.g. 5-fold or 10-fold), where the data are split into
<em>k</em> portions, the <em>k</em>th portion is held out for validation
and the procedure is repeated <em>k</em> times. If these folds are
stratified in geographic or environmental space, we talk of spatial
block cross-validation and environmental block cross-validation <span
class="citation">(Roberts et al. 2017)</span>.</p>
<p>Here, we will use the function <code>crossvalSDM()</code> from the
<code>mecofun</code> package to split our data into 5 folds,
re-calibrate the model using only 4/5 of the original data and predict
the model to the hold-out 1/5 of the data.</p>
<pre class="r"><code>preds_cv &lt;- crossvalSDM(m_step, traindat = lynx_thinned, colname_species = &#39;occ&#39;, colname_pred = my_preds)</code></pre>
<p>We receive a numeric vector of cross-validated prediction. Out of
curiosity, let us compare the fitted values on the training data and the
predictions on the cross-validation data. You should see that the
predictions are similar (this is good, otherwise our model would be very
sensitive to changes in input data) but not identical - thus, we have
basically added some noise.</p>
<pre class="r"><code>plot(m_step$fitted.values, preds_cv, xlab=&#39;Fitted values&#39;, ylab=&#39;Predicted values from CV&#39;)
abline(0,1,col=&#39;red&#39;,lwd=2)</code></pre>
<p><img src="5_SDM_eval_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We will use these cross-validated predictions to assess model
performance.</p>
<div id="threshold-dependent-performance-measures"
class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Threshold-dependent
performance measures</h3>
<p>Now, we want to know how well our model predicts the observations.
Different measures are available for quantifying this. A lot of these
measures are threshold-dependent. You have probably realised that our
model predicts a continuous response, the probability of occurrence,
while our observations are binary. Many performance measures rely on
comparisons like “How many presence observations does the model
correctly predict as presence”. In order to answer this we first need to
convert the continuous probabilities into binary predictions. Different
thresholds are introduced in <span class="citation">Liu et al.
(2005)</span>. Most of these are implemented in the
<code>PresenceAbsence</code> package in the
<code>optimal.thresholds</code> function.</p>
<pre class="r"><code>library(PresenceAbsence)

# We first prepare our data:
# Prepare cross-validated predictions:
thresh_dat &lt;- data.frame(
  ID = seq_len(nrow(lynx_thinned)), 
    obs = lynx_thinned$occ,
    pred = preds_cv)
        
# Then, we find the optimal thresholds:     
(thresh_cv &lt;- PresenceAbsence::optimal.thresholds(DATA= thresh_dat))</code></pre>
<pre><code>##          Method      pred
## 1       Default 0.5000000
## 2     Sens=Spec 0.4550000
## 3  MaxSens+Spec 0.3950000
## 4      MaxKappa 0.4400000
## 5        MaxPCC 0.4400000
## 6  PredPrev=Obs 0.4700000
## 7       ObsPrev 0.4489164
## 8      MeanProb 0.4474578
## 9    MinROCdist 0.4400000
## 10      ReqSens 0.4600000
## 11      ReqSpec 0.4300000
## 12         Cost 0.4400000</code></pre>
<p>We can compare observed vs. predicted presences and absences based on
these tresholds. For this, we take our predictions from the
cross-validation. The comparison is easiest illustrated in a confusion
matrix, for example using the function <code>cmx</code> in the
<code>PresenceAbsence</code> package.</p>
<p>Have a look at <span class="citation">Liu et al. (2005)</span> to see
which thresholds they recommend. Here, we will use the threshold that
maximises the sum of sensitivity and specificity (the third row in the
thresholds data frame):</p>
<pre class="r"><code>(cmx_maxSSS &lt;- PresenceAbsence::cmx(DATA= thresh_dat, threshold=thresh_cv[3,2]))</code></pre>
<pre><code>##          observed
## predicted   1   0
##         1 134  31
##         0  11 147</code></pre>
<p>From such a confusion matrix, we can calculate different evaluation
criteria. For example,<br />
- the proportion of correctly classified test observations
<code>pcc</code><br />
- the proportion of correctly classified presences, also called
sensitivity or true positive rate<br />
- the proportion of correctly classified absences, also called
specificity or true negative rate</p>
<pre class="r"><code># Proportion of correctly classified observations
PresenceAbsence::pcc(cmx_maxSSS, st.dev=F)</code></pre>
<pre><code>## [1] 0.869969</code></pre>
<pre class="r"><code># Sensitivity = true positive rate
PresenceAbsence::sensitivity(cmx_maxSSS, st.dev=F)</code></pre>
<pre><code>## [1] 0.9241379</code></pre>
<pre class="r"><code># Specificity = true negative rate
PresenceAbsence::specificity(cmx_maxSSS, st.dev=F)</code></pre>
<pre><code>## [1] 0.8258427</code></pre>
<p>Other measures are <em>Kappa</em> and <em>TSS</em> (the true skill
statistic). <span class="citation">Allouche, Tsoar, and Kadmon
(2006)</span> explain how to calculate these.</p>
<pre class="r"><code># Kappa
PresenceAbsence::Kappa(cmx_maxSSS, st.dev=F)</code></pre>
<pre><code>## [1] 0.7405126</code></pre>
<pre class="r"><code># True skill statistic
TSS(cmx_maxSSS) </code></pre>
<pre><code>## [1] 0.7499806</code></pre>
<p>According to <span class="citation">Araujo et al. (2005)</span>,
<em>Kappa</em>&gt;0.4 indicate good predictions. For TSS, we often
assume TSS&gt;0.5 to indicate good predictions.</p>
</div>
<div id="threshold-independent-performance-measures"
class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span>
Threshold-independent performance measures</h3>
<p>The most common evaluation statistic that avoids thresholding the
data is AUC - the area under the receiver-operating characteristic (ROC)
curve. ROC curves are generated by calculating sensitivity (true
positive rate) and specificity (true negative rate) for many thresholds
along the entire range of predicted probabilities. Then, (1-specificity)
is plotted on the x-axis against sensitivity on the y axis. The area
under this curve is called the AUC. The further the generated curve
deviates from the 1:1 line towards the upper-left corner, the better the
model predicts presence/absence of a species. If we would take a random
presence and a random absence from our observations and make
predictions, than AUC can be interpeted as the chance of assigning a
higher predicted occurrence probability to the presence compared to the
absence point. Typically, we regard AUC&gt;0.7 as indicating fair
predictions <span class="citation">(Araujo et al. 2005)</span>.</p>
<pre class="r"><code>library(AUC)

# Let&#39;s have a look a the ROC curve:
roc_cv &lt;- roc(preds_cv, as.factor(lynx_thinned$occ))
plot(roc_cv, col = &quot;grey70&quot;, lwd = 2)</code></pre>
<p><img src="5_SDM_eval_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code># Compute the AUC:
AUC::auc(roc_cv)</code></pre>
<pre><code>## [1] 0.9130182</code></pre>
<p>It seems our model is performing pretty well on hold-out data. We can
thus attempt making predictions in space and time.</p>
<p>Please be aware that many packages contain functions for evaluating
SDMs. As always you have to find your own way. Here, I provide merely
examples. To ease further performance assessments during this course,
the <code>mecofun</code> package contains a function
<code>evalSDM()</code> that computes the here-mentioned performance
measures. By default, this function uses the <em>MaxSens+Spec</em>
threshold.</p>
<pre class="r"><code>evalSDM(lynx_thinned$occ, preds_cv)</code></pre>
<pre><code>##         AUC       TSS     Kappa      Sens      Spec      PCC        D2 thresh
## 1 0.9130182 0.7499806 0.7405126 0.9241379 0.8258427 0.869969 0.4094901  0.395</code></pre>
<div class="alert alert-info">
<p><em><strong>Exercise:</strong></em></p>
<p>Use the <code>m2</code> model from the previous exercise and assess
the TSS, sensitivity, specificity, and AUC.</p>
<ul>
<li>Which model (<code>m1</code> or <code>m2</code>) has a higher
predictive performance?</li>
</ul>
</div>
</div>
</div>
</div>
<div id="spatio-temporal-predictions" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Spatio-temporal
predictions</h1>
<p>We have already learned how to make predictions using the function
<code>predict()</code> and also using the argument <code>newdata</code>.
All we need for transferring our model to other places and times are the
respective environmental variables.</p>
<div id="prepare-the-environmental-layers" class="section level2"
number="3.1">
<h2><span class="header-section-number">3.1</span> Prepare the
environmental layers</h2>
<p>We have already learned how to download climate data from <a
href="http://www.worldclim.org/">WorldClim</a> using the
<code>geodata</code> package.</p>
<p>Download the current climate layers:</p>
<pre class="r"><code>library(geodata)

# rough European extent
extent_eur &lt;- c(-15,45,35,72)

# Please note that you have to set download=T if you haven&#39;t downloaded the data before:
bio_curr &lt;- geodata::worldclim_global(var = &#39;bio&#39;, res = 10, download = F, path = &#39;data&#39;)

# Crop data to European extent
bio_curr &lt;- terra::crop(bio_curr, extent_eur)

# Aggregate to 1° resolution
bio_curr &lt;- terra::aggregate(bio_curr, 6)

# Change names such that they match the names in our lynx data set
names(bio_curr) &lt;- c(paste0(&#39;bio0&#39;,1:9), paste0(&#39;bio&#39;,10:19))</code></pre>
<p>Download the future climate layers. More information on the model
abbreviations and the available SSPs can be found here: <a
href="https://www.worldclim.org/data/cmip6/cmip6_clim10m.html"
class="uri">https://www.worldclim.org/data/cmip6/cmip6_clim10m.html</a>.</p>
<pre class="r"><code># Please note that you have to set download=T if you haven&#39;t downloaded the data before:
bio_fut &lt;- geodata::cmip6_world(model=&#39;ACCESS-ESM1-5&#39;, ssp=&#39;245&#39;, time=&#39;2041-2060&#39;, var=&#39;bioc&#39;, download=F, res=10, path=&#39;data&#39;)

# Crop data to European extent
bio_fut &lt;- terra::crop(bio_fut, extent_eur)

# Aggregate to 1° resolution
bio_fut &lt;- terra::aggregate(bio_fut, 6)</code></pre>
</div>
<div id="make-predictions-to-the-environmental-layers"
class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Make predictions to
the environmental layers</h2>
<p>It is now straight forward to make continuous predictions to the
current and the future climate:</p>
<pre class="r"><code># Prepare data frames
bio_curr_df &lt;- data.frame(crds(bio_curr),as.points(bio_curr))
bio_fut_df &lt;- data.frame(crds(bio_fut),as.points(bio_fut))

# Make continuous predictions:
bio_curr_df$pred_glm &lt;- predict(m_step, newdata= bio_curr_df, type=&quot;response&quot;)
bio_fut_df$pred_glm &lt;- predict(m_step, newdata= bio_fut_df, type=&quot;response&quot;)</code></pre>
<p>Of course, we can also plot the predictions:</p>
<pre class="r"><code>par(mfrow=c(1,2))

# Make raster of predictions to current environment:
r_pred_curr &lt;- terra::rast(bio_curr_df[,c(&#39;x&#39;,&#39;y&#39;,&#39;pred_glm&#39;)], type=&#39;xyz&#39;, crs=crs(bio_curr))
plot(r_pred_curr, axes=F, main=&#39;Occ. prob. - today&#39;)

# Make raster stack of predictions to future environment:
r_pred_fut &lt;- terra::rast(bio_fut_df[,c(&#39;x&#39;,&#39;y&#39;,&#39;pred_glm&#39;)], type=&#39;xyz&#39;, crs=crs(bio_curr))
plot(r_pred_fut, axes=F, main=&#39;Occ. prob. - 2050&#39;)</code></pre>
<p><img src="5_SDM_eval_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>We see that lynx is predicted to decline under the chosen climate
scenario.</p>
<p>Lastly, we can also translate the continuous predictions into binary
predictions and plot the resulting maps.</p>
<pre class="r"><code># Make binary predictions:
bio_curr_df$bin_glm &lt;- ifelse(bio_curr_df$pred_glm &gt;= thresh_cv[3,2], 1, 0)
bio_fut_df$bin_glm &lt;- ifelse(bio_fut_df$pred_glm &gt;= thresh_cv[3,2], 1, 0)

# Make raster stack of predictions to current environment:
r_pred_curr &lt;- terra::rast(bio_curr_df[,c(&#39;x&#39;,&#39;y&#39;,&#39;pred_glm&#39;,&#39;bin_glm&#39;)], type=&#39;xyz&#39;, crs=crs(bio_curr))
plot(r_pred_curr, axes=F)</code></pre>
<p><img src="5_SDM_eval_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code># Make raster stack of predictions to future environment:
r_pred_fut &lt;- terra::rast(bio_fut_df[,c(&#39;x&#39;,&#39;y&#39;,&#39;pred_glm&#39;,&#39;bin_glm&#39;)], type=&#39;xyz&#39;, crs=crs(bio_curr))
plot(r_pred_fut, axes=F)</code></pre>
<p><img src="5_SDM_eval_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<div class="alert alert-info">
<p><em><strong>Exercise:</strong></em></p>
<p>Use the <code>m2</code>model from the previous exercise and make
predictions to current and future climates.</p>
<ul>
<li>Which model predicts a larger area under current climate?</li>
<li>Which model predicts a larger difference between current and future
climate?</li>
</ul>
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-allouche2006" class="csl-entry">
Allouche, O., A. Tsoar, and R. Kadmon. 2006. <span>“Assessing the
Accuracy of Species Distribution Models: Prevalence, Kappa and the True
Skill Statistic (TSS).”</span> <em>Journal of Applied Ecology</em> 43:
1223–32.
</div>
<div id="ref-araujo2005" class="csl-entry">
Araujo, M. B., R. G. Pearson, W. Thuiller, and M. Erhard. 2005.
<span>“Validation of Species-Climate Impact Models Under Climate
Change.”</span> <em>Global Change Biology</em> 11: 1504–13.
</div>
<div id="ref-Elith2009" class="csl-entry">
Elith, J., and J. R. Leathwick. 2009. <span>“Species Distribution
Models: Ecological Explanation and Prediction Across Space and
Time.”</span> <em>Annual Review of Ecology, Evolution, and
Systematics</em> 40: 677–97.
</div>
<div id="ref-Franklin2010" class="csl-entry">
Franklin, J. 2010. <em>Mapping Species Distributions: Spatial Inference
and Prediction</em>. Cambride University Press.
</div>
<div id="ref-guisan2005" class="csl-entry">
Guisan, A., and W. Thuiller. 2005. <span>“Predicting Species
Distribution: Offering More Than Simple Habitat Models.”</span>
<em>Ecology Letters</em> 8: 993–1009.
</div>
<div id="ref-Guisan2017" class="csl-entry">
Guisan, A., W. Thuiller, and N. E. Zimmermann. 2017. <em>Habitat
Suitability and Distribution Models with Applications in r</em>.
Cambride University Press.
</div>
<div id="ref-Guisan2000" class="csl-entry">
Guisan, A., and N. E. Zimmermann. 2000. <span>“Predictive Habitat
Distribution Models in Ecology.”</span> <em>Ecological Modelling</em>
135: 147–86.
</div>
<div id="ref-Liu2005" class="csl-entry">
Liu, C., P. M. Berry, T. P. Dawson, and R. G. Pearson. 2005.
<span>“Selecting Thresholds of Occurrence in the Prediction of Species
Distributions.”</span> <em>Ecography</em> 28: 385–93.
</div>
<div id="ref-Roberts2017" class="csl-entry">
Roberts, D. R., V. Bahn, S. Ciuti, M. S. Boyce, J. Elith, G.
Guillera-Arroita, S: Hauenstein, et al. 2017. <span>“Cross-Validation
Strategies for Data with Temporal, Spatial, Hierarchical, or
Phylogenetic Structure.”</span> <em>Ecography</em> 40: 913–29.
</div>
<div id="ref-Zurell2020a" class="csl-entry">
Zurell, D., J. Franklin, C. König, P. J. Bouchet, C. F. Dormann, J.
Elith, G. Fandos, et al. 2020. <span>“A Standard Protocol for Reporting
Species Distribution Models.”</span> <em>Ecography</em> 43 (9): 1261–77.
<a
href="https://doi.org/10.1111/ecog.04960">https://doi.org/10.1111/ecog.04960</a>.
</div>
</div>
</div>

<!DOCTYPE html>
<html>

<br>
<hr />
<div id="footer">
<p>Damaris Zurell 2023 <a href="http://creativecommons.org/licenses/by/4.0/" >(CC BY 4.0)</a>.  </p>
</div>

</html>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
